{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aplicacoes_analise_sentimentos_bertimbau.ipynb","provenance":[{"file_id":"1xDAMSM4Y_2Sr3bWG6PhhJYomCVgksDeO","timestamp":1659735797502},{"file_id":"1K4x6-3xy6ZCdB2WfHL0nLROHH9rCN_Tu","timestamp":1659636335686},{"file_id":"1jAYSCgbnI3wSml2jidjKbzXH7fOjw938","timestamp":1649186537204},{"file_id":"1RLf_G6xNJKreqJDdPo8i2BXkedohDOTi","timestamp":1648857149793},{"file_id":"1Z61I2TZ0b9EAm6v2WB0HhTMgy6DZh6Ya","timestamp":1604089654428}],"mount_file_id":"1O_lTvjIaMVlS0_AqBlJg-NqLW2JAVBxq","authorship_tag":"ABX9TyMReJA/QOaizWe1ySmUhDFk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"wFQECRjVZSLh"},"source":["# Processamento de Linguagem Natural - Minicurso do SBBD 2022"]},{"cell_type":"markdown","metadata":{"id":"UAxVLEc9ZPo_"},"source":["# Categorização de textos - Análise de sentimentos (polaridade)\n","\n","Esse código foi desenvolvido para o minicurso de PLN no SBBD 2022.\n","\n","Autoras: Helena Caseli, Cláudia Freitas e Roberta Viola\n","\n","https://sites.google.com/view/brasileiras-pln/\n","\n","Fontes:\n","* Curso de Linguística Computacional da UFMG - Prof. Thiago Castro Ferreira https://www.youtube.com/playlist?list=PLLrlHSmC0Mw73a1t73DEjgGMPyu8QssWT => Esse código está baseado na aula 7.6\n","* https://huggingface.co/\n","* https://github.com/neuralmind-ai/portuguese-bert\n","* https://pytorch.org/\n","\n","Esse código:\n","* Utiliza um modelo neural pré-treinado (BERTimbau), baseado na arquitetura Transformer, com fine-tuning para categorização de texto.\n","* Categorização de uma review como positiva, neutra ou negativa.\n","\n","Dataset/corpus:\n","* B2WReviews-1: https://github.com/b2w-digital/b2w-reviews01"]},{"cell_type":"markdown","source":["**IMPORTANTE:** Setar a GPU do Colab."],"metadata":{"id":"rzsnS5YUMYR7"}},{"cell_type":"markdown","source":["## Instalando as dependências"],"metadata":{"id":"2-R_vD5qE1lv"}},{"cell_type":"code","source":["!pip3 install transformers"],"metadata":{"id":"vJCyk66PEV3J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lwa837tUO5Pg"},"source":["## Baixando o *corpus*"]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/b2w-digital/b2w-reviews01/main/B2W-Reviews01.csv"],"metadata":{"id":"CNQ7MQeoM9X7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Carregando o *corpus*\n","\n","Se quiser diminuir o tempo de processamento, basta carregar um número menor de instâncias (por exemplo, apenas 10000 instâncias do *corpus* B2W Reviews)."],"metadata":{"id":"PLC5pGw9NMct"}},{"cell_type":"code","source":["import pandas as pd\n","corpus = pd.read_csv('B2W-Reviews01.csv', sep=',', quotechar='\\\"', nrows=10000)\n","\n","corpus.dropna(subset=[\"review_text\"],inplace=True)\n","\n","corpus.head(1)\n"],"metadata":{"id":"V52M2AuWyu8V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Separando apenas as informações que vamos usar no *fine-tuning* que será realizado com base apenas no texto das revisões de produto (```review_text```).\n","\n","Vamos também fazer um processamento nas informações originais de ```overall_rating``` agrupando as notas 4 e 5 na classe positiva (2), nota 3 na classe neutra (1) e notas 1 e 2 na classe negativa (0)."],"metadata":{"id":"r661cr1Y8BWX"}},{"cell_type":"code","source":["df = corpus.loc[:, (\"review_text\", \"overall_rating\")]\n","df[\"label\"] = corpus[\"overall_rating\"].replace({5: 2, 4: 2, 3: 1, 2: 0, 1: 0})\n","df.head(20)"],"metadata":{"id":"Q7I9sLfq55At","colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"status":"ok","timestamp":1660152059903,"user_tz":180,"elapsed":238,"user":{"displayName":"Profa. Helena Caseli","userId":"17145953999266413220"}},"outputId":"75d5b7e1-b538-491d-e124-099de4810729"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          review_text  overall_rating  label\n","0   Estou contente com a compra entrega rápida o ú...               4      2\n","1   Por apenas R$1994.20,eu consegui comprar esse ...               4      2\n","2   SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...               4      2\n","3   MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...               4      2\n","4   A entrega foi no prazo, as americanas estão de...               5      2\n","5   Excelente produto, por fora em material acríli...               5      2\n","6   produto mto bom, com essa garrafinha vc pode a...               5      2\n","7   Produto excelente qualidade boa câmera desenvo...               4      2\n","8   O barulho e minimo e o vento é bem forte na ve...               5      2\n","9   MEU PRODUTO NAO FOI ENTREGUE E A AMERICANAS ES...               1      0\n","10  Comprei um pra cara membro da família nesse na...               5      2\n","11  Produto maravilhoso! Não é barulhento, fácil m...               5      2\n","12  Jogo de panelas excelente material, entrega a ...               5      2\n","13  Esse celular não vale nada a bateria não vale ...               1      0\n","14  produto de acordo com o que eu esperava. Somen...               5      2\n","15  a mochila nao esta fechando direito por isso n...               2      0\n","16  Produto condizente com o anúncio, com material...               4      2\n","17  nao tenho nada a reclamar, bom atendimento e e...               4      2\n","18  Até agora os produtos que eu comprei foram ent...               4      2\n","19  A capa não decepcionou por enquanto, mas a bol...               3      1"],"text/html":["\n","  <div id=\"df-1d6ee79d-9e00-4659-96cd-3164ce49fce1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review_text</th>\n","      <th>overall_rating</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Estou contente com a compra entrega rápida o ú...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Por apenas R$1994.20,eu consegui comprar esse ...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>SUPERA EM AGILIDADE E PRATICIDADE OUTRAS PANEL...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>MEU FILHO AMOU! PARECE DE VERDADE COM TANTOS D...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A entrega foi no prazo, as americanas estão de...</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Excelente produto, por fora em material acríli...</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>produto mto bom, com essa garrafinha vc pode a...</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Produto excelente qualidade boa câmera desenvo...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>O barulho e minimo e o vento é bem forte na ve...</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>MEU PRODUTO NAO FOI ENTREGUE E A AMERICANAS ES...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Comprei um pra cara membro da família nesse na...</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Produto maravilhoso! Não é barulhento, fácil m...</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Jogo de panelas excelente material, entrega a ...</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Esse celular não vale nada a bateria não vale ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>produto de acordo com o que eu esperava. Somen...</td>\n","      <td>5</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>a mochila nao esta fechando direito por isso n...</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Produto condizente com o anúncio, com material...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>nao tenho nada a reclamar, bom atendimento e e...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Até agora os produtos que eu comprei foram ent...</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>A capa não decepcionou por enquanto, mas a bol...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d6ee79d-9e00-4659-96cd-3164ce49fce1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1d6ee79d-9e00-4659-96cd-3164ce49fce1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1d6ee79d-9e00-4659-96cd-3164ce49fce1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["Vamos dar uma olhada na distribuição das instâncias nas 3 classes resultantes."],"metadata":{"id":"DciZKbHX8PC8"}},{"cell_type":"code","source":["df.groupby(['label']).size().plot.bar()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"hPFmk1OP5G3y","executionInfo":{"status":"ok","timestamp":1660152074175,"user_tz":180,"elapsed":251,"user":{"displayName":"Profa. Helena Caseli","userId":"17145953999266413220"}},"outputId":"7fd1176f-903c-44db-adc1-e6d3c90e0f41"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7efb800886d0>"]},"metadata":{},"execution_count":17},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["## Separando o *corpus* em treinamento e teste\n","\n","Vamos separar 80% para treinamento e deixar 20% para teste."],"metadata":{"id":"DdYRA0FbNS24"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","treino, teste = train_test_split(df, stratify=df[\"label\"], test_size=0.2) #train_size=0.8, shuffle=True)\n","\n","len(treino), len(teste)"],"metadata":{"id":"utrcLnYNNXJa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660152074439,"user_tz":180,"elapsed":8,"user":{"displayName":"Profa. Helena Caseli","userId":"17145953999266413220"}},"outputId":"75979bfd-c508-464c-b0a9-b23493a68398"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7772, 1944)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["treino.groupby(['label']).size().plot.bar()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"J8kwbvLu7yKw","executionInfo":{"status":"ok","timestamp":1660152074860,"user_tz":180,"elapsed":427,"user":{"displayName":"Profa. Helena Caseli","userId":"17145953999266413220"}},"outputId":"aed0393f-90a3-4608-9d82-6476cb6ff667"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7efb80379110>"]},"metadata":{},"execution_count":19},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["## Preparando o *fine-tuning*\n","\n"],"metadata":{"id":"O65_Q8Z1NcC9"}},{"cell_type":"code","source":["def get_features(info):   \n","  x = info[\"review_text\"] \n","  y = info[\"label\"]\n","  return { 'X': x, 'y': y }"],"metadata":{"id":"jhHNURZkJd96"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Importando dependências.\n","\n","Para mais informações sobre essas dependências/bibliotecas, consulte: https://github.com/neuralmind-ai/portuguese-bert"],"metadata":{"id":"GiVwOhU4Je3K"}},{"cell_type":"code","source":["import os\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.metrics import accuracy_score, f1_score, classification_report  "],"metadata":{"id":"aGQZqVNYNlR6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Setando os parâmetros da rede neural."],"metadata":{"id":"cg0-l3bqGmyO"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","nclasses = 3\n","nepochs = 5\n","batch_size = 8\n","batch_status = 32\n","learning_rate = 1e-5  # usar uma bem baixa para o caso dos modelos pré-treinados\n","early_stop = 2        # se em duas épocas consecutivas o resultado não melhorar no conjunto de validação, então para o treinamento\n","\n","max_length = 180      # todas as sequências com mais de 180 tokens serão truncadas para considerar só os 180 primeiros\n","write_path = 'model'"],"metadata":{"id":"sGOWjXDTGqNO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instanciando tokenizador, modelo, função de erro, otimizador e dados de treinamento e teste em lotes"],"metadata":{"id":"7xqSEBxYG8du"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, Dataset\n","\n","traindata = DataLoader([get_features(treino.iloc[i]) for i in range(len(treino))], batch_size=batch_size, shuffle=True)\n","testdata = DataLoader([get_features(teste.iloc[i]) for i in range(len(teste))], batch_size=batch_size, shuffle=True)"],"metadata":{"id":"b7BeVdIaKNPe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aqui, como estamos usando os textos das *reviews* como a *feature* para o *fine-tuning*, vamos usar o modelo pré-treinado do BERTimbau para SequenceClassification."],"metadata":{"id":"RWdWnc25UTie"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n","model = AutoModelForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=nclasses).to(device)\n","\n","optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"],"metadata":{"id":"Kq7qSSFOG_en","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660152078571,"user_tz":180,"elapsed":2950,"user":{"displayName":"Profa. Helena Caseli","userId":"17145953999266413220"}},"outputId":"d5c1d099-2182-4c60-ae1a-b2607063dcca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["Definindo o método de avaliação.\n","\n","Veja que aqui fazemos a chamada ao ```tokenizer``` do BERT. Esse tokenizador transforma a sentença de entrada para o formato esperado pelo BERT nessa tarefa, que é inserindo um *token* [CLS] no início da sentença e [SEP] ao final. Esse é o formato esperado pelo BERT para tarefas de *single text classification*.\n","\n","Outro processamento feito no código abaixo é setar o ```padding``` que completa (com o *token* especial [PAD]) ou trunca a sequência de *tokens* para ficar dentro do limite estabelecido (```max_length```).\n","\n","A saída da classificação (em ```output```) é uma *embedding* associada ao *token* [CLS]."],"metadata":{"id":"tv-8OdsIHLVV"}},{"cell_type":"code","source":["def evaluate(model, testdata):\n","  model.eval()\n","  y_real, y_pred = [], []\n","  for batch_idx, inp in enumerate(testdata):\n","    texts, labels = inp['X'], inp['y']\n","    \n","    # classificando\n","    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n","    output = model(**inputs) # output contém as probabilidades da entrada pertencer a uma das classes\n","                \n","    pred_labels = torch.argmax(output.logits, 1)\n","    \n","    y_real.extend(labels.tolist())\n","    y_pred.extend(pred_labels.tolist())\n","    \n","    if (batch_idx+1) % batch_status == 0:\n","      print('Progresso:', round(batch_idx / len(testdata), 2), batch_idx)\n","  \n","  print(classification_report(y_real, y_pred, labels=[0, 1, 2], target_names=['Negativa', 'Neutra', 'Positiva']))\n","  f1 = f1_score(y_real, y_pred, average='weighted')\n","  acc = accuracy_score(y_real, y_pred)\n","  return f1, acc"],"metadata":{"id":"VxsNr7izHFpE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Treinamento"],"metadata":{"id":"il5tkdcDfrkM"}},{"cell_type":"markdown","source":["**IMPORTANTE:** Setar a GPU do Colab."],"metadata":{"id":"Y9X2OncHLqgQ"}},{"cell_type":"code","source":["max_f1, repeat = 0, 0\n","for epoch in range(nepochs):\n","  model.train()\n","  f1, acc = evaluate(model, testdata)\n","  losses = []\n","  for batch_idx, inp in enumerate(traindata):\n","    texts, labels = inp['X'], inp['y']\n","\n","    # classificando\n","    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n","    output = model(**inputs, labels=labels.to(device))\n","\n","    # Calculando a loss (erro)\n","    loss = output.loss\n","    losses.append(float(loss))\n","\n","    # Backpropagation com base no erro\n","    loss.backward()\n","    optimizer.step()\n","    optimizer.zero_grad()\n","\n","    # Informando o andamneto\n","    if (batch_idx+1) % batch_status == 0:\n","      print('Epoca de treinamento: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTotal Loss: {:.6f}'.format(epoch, \\\n","        batch_idx+1, len(traindata), 100. * batch_idx / len(traindata), \n","        float(loss), round(sum(losses) / len(losses), 5)))\n","  \n","  f1, acc = evaluate(model, testdata)\n","  print('F1: ', f1, 'Accuracy: ', acc)\n","  if f1 > max_f1:\n","    model.save_pretrained(os.path.join(write_path, 'model'))\n","    max_f1 = f1\n","    repeat = 0\n","    print('Salvando o melhor modelo ...')\n","  else:\n","    repeat += 1\n","  \n","  if repeat == early_stop:\n","    break"],"metadata":{"id":"o288EGY3fvrP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"53539fe6-814c-4967-e3cc-5b95822d9232","executionInfo":{"status":"ok","timestamp":1660152735565,"user_tz":180,"elapsed":656999,"user":{"displayName":"Profa. Helena Caseli","userId":"17145953999266413220"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Progress: 0.13 31\n","Progress: 0.26 63\n","Progress: 0.39 95\n","Progress: 0.52 127\n","Progress: 0.65 159\n","Progress: 0.79 191\n","Progress: 0.92 223\n","              precision    recall  f1-score   support\n","\n","    Negativa       0.33      0.00      0.01       476\n","      Neutra       0.00      0.00      0.00       240\n","    Positiva       0.63      1.00      0.77      1228\n","\n","    accuracy                           0.63      1944\n","   macro avg       0.32      0.33      0.26      1944\n","weighted avg       0.48      0.63      0.49      1944\n","\n","Train Epoch: 0 [32/972 (3%)]\tLoss: 0.820846\tTotal Loss: 0.876210\n","Train Epoch: 0 [64/972 (6%)]\tLoss: 0.765200\tTotal Loss: 0.803460\n","Train Epoch: 0 [96/972 (10%)]\tLoss: 0.466072\tTotal Loss: 0.720650\n","Train Epoch: 0 [128/972 (13%)]\tLoss: 0.146775\tTotal Loss: 0.633540\n","Train Epoch: 0 [160/972 (16%)]\tLoss: 0.202871\tTotal Loss: 0.611720\n","Train Epoch: 0 [192/972 (20%)]\tLoss: 0.127982\tTotal Loss: 0.593530\n","Train Epoch: 0 [224/972 (23%)]\tLoss: 0.512627\tTotal Loss: 0.579010\n","Train Epoch: 0 [256/972 (26%)]\tLoss: 0.514885\tTotal Loss: 0.570190\n","Train Epoch: 0 [288/972 (30%)]\tLoss: 0.164237\tTotal Loss: 0.558390\n","Train Epoch: 0 [320/972 (33%)]\tLoss: 0.225624\tTotal Loss: 0.544880\n","Train Epoch: 0 [352/972 (36%)]\tLoss: 0.566691\tTotal Loss: 0.531540\n","Train Epoch: 0 [384/972 (39%)]\tLoss: 0.087942\tTotal Loss: 0.515420\n","Train Epoch: 0 [416/972 (43%)]\tLoss: 0.139377\tTotal Loss: 0.509240\n","Train Epoch: 0 [448/972 (46%)]\tLoss: 0.676805\tTotal Loss: 0.508940\n","Train Epoch: 0 [480/972 (49%)]\tLoss: 0.699901\tTotal Loss: 0.500770\n","Train Epoch: 0 [512/972 (53%)]\tLoss: 0.509079\tTotal Loss: 0.497320\n","Train Epoch: 0 [544/972 (56%)]\tLoss: 0.129139\tTotal Loss: 0.492560\n","Train Epoch: 0 [576/972 (59%)]\tLoss: 0.438865\tTotal Loss: 0.482680\n","Train Epoch: 0 [608/972 (62%)]\tLoss: 0.199149\tTotal Loss: 0.478230\n","Train Epoch: 0 [640/972 (66%)]\tLoss: 0.505479\tTotal Loss: 0.472680\n","Train Epoch: 0 [672/972 (69%)]\tLoss: 0.172573\tTotal Loss: 0.465820\n","Train Epoch: 0 [704/972 (72%)]\tLoss: 0.291933\tTotal Loss: 0.468890\n","Train Epoch: 0 [736/972 (76%)]\tLoss: 0.435637\tTotal Loss: 0.465990\n","Train Epoch: 0 [768/972 (79%)]\tLoss: 0.329634\tTotal Loss: 0.463270\n","Train Epoch: 0 [800/972 (82%)]\tLoss: 0.353603\tTotal Loss: 0.462050\n","Train Epoch: 0 [832/972 (85%)]\tLoss: 0.212943\tTotal Loss: 0.458200\n","Train Epoch: 0 [864/972 (89%)]\tLoss: 0.110017\tTotal Loss: 0.452720\n","Train Epoch: 0 [896/972 (92%)]\tLoss: 0.383654\tTotal Loss: 0.452540\n","Train Epoch: 0 [928/972 (95%)]\tLoss: 0.196565\tTotal Loss: 0.452050\n","Train Epoch: 0 [960/972 (99%)]\tLoss: 0.433978\tTotal Loss: 0.448790\n","Progress: 0.13 31\n","Progress: 0.26 63\n","Progress: 0.39 95\n","Progress: 0.52 127\n","Progress: 0.65 159\n","Progress: 0.79 191\n","Progress: 0.92 223\n","              precision    recall  f1-score   support\n","\n","    Negativa       0.86      0.92      0.89       476\n","      Neutra       0.57      0.20      0.30       240\n","    Positiva       0.88      0.97      0.92      1228\n","\n","    accuracy                           0.86      1944\n","   macro avg       0.77      0.70      0.70      1944\n","weighted avg       0.84      0.86      0.84      1944\n","\n","F1:  0.8363096164463775 Accuracy:  0.86059670781893\n","Saving best model...\n","Progress: 0.13 31\n","Progress: 0.26 63\n","Progress: 0.39 95\n","Progress: 0.52 127\n","Progress: 0.65 159\n","Progress: 0.79 191\n","Progress: 0.92 223\n","              precision    recall  f1-score   support\n","\n","    Negativa       0.86      0.92      0.89       476\n","      Neutra       0.57      0.20      0.30       240\n","    Positiva       0.88      0.97      0.92      1228\n","\n","    accuracy                           0.86      1944\n","   macro avg       0.77      0.70      0.70      1944\n","weighted avg       0.84      0.86      0.84      1944\n","\n","Train Epoch: 1 [32/972 (3%)]\tLoss: 0.214808\tTotal Loss: 0.256490\n","Train Epoch: 1 [64/972 (6%)]\tLoss: 0.162411\tTotal Loss: 0.279910\n","Train Epoch: 1 [96/972 (10%)]\tLoss: 0.080468\tTotal Loss: 0.272400\n","Train Epoch: 1 [128/972 (13%)]\tLoss: 0.405792\tTotal Loss: 0.282430\n","Train Epoch: 1 [160/972 (16%)]\tLoss: 0.121281\tTotal Loss: 0.273460\n","Train Epoch: 1 [192/972 (20%)]\tLoss: 0.587909\tTotal Loss: 0.284320\n","Train Epoch: 1 [224/972 (23%)]\tLoss: 0.100451\tTotal Loss: 0.284970\n","Train Epoch: 1 [256/972 (26%)]\tLoss: 0.826435\tTotal Loss: 0.281740\n","Train Epoch: 1 [288/972 (30%)]\tLoss: 0.134602\tTotal Loss: 0.282910\n","Train Epoch: 1 [320/972 (33%)]\tLoss: 0.075831\tTotal Loss: 0.287900\n","Train Epoch: 1 [352/972 (36%)]\tLoss: 0.242629\tTotal Loss: 0.286990\n","Train Epoch: 1 [384/972 (39%)]\tLoss: 0.798644\tTotal Loss: 0.284730\n","Train Epoch: 1 [416/972 (43%)]\tLoss: 0.371072\tTotal Loss: 0.281780\n","Train Epoch: 1 [448/972 (46%)]\tLoss: 0.067025\tTotal Loss: 0.281790\n","Train Epoch: 1 [480/972 (49%)]\tLoss: 0.302451\tTotal Loss: 0.281520\n","Train Epoch: 1 [512/972 (53%)]\tLoss: 0.275003\tTotal Loss: 0.286400\n","Train Epoch: 1 [544/972 (56%)]\tLoss: 0.162462\tTotal Loss: 0.282270\n","Train Epoch: 1 [576/972 (59%)]\tLoss: 0.227618\tTotal Loss: 0.283560\n","Train Epoch: 1 [608/972 (62%)]\tLoss: 0.150381\tTotal Loss: 0.285610\n","Train Epoch: 1 [640/972 (66%)]\tLoss: 0.674608\tTotal Loss: 0.288190\n","Train Epoch: 1 [672/972 (69%)]\tLoss: 0.050194\tTotal Loss: 0.286240\n","Train Epoch: 1 [704/972 (72%)]\tLoss: 0.490125\tTotal Loss: 0.288550\n","Train Epoch: 1 [736/972 (76%)]\tLoss: 0.821498\tTotal Loss: 0.288690\n","Train Epoch: 1 [768/972 (79%)]\tLoss: 0.471389\tTotal Loss: 0.288570\n","Train Epoch: 1 [800/972 (82%)]\tLoss: 0.128648\tTotal Loss: 0.287660\n","Train Epoch: 1 [832/972 (85%)]\tLoss: 0.240755\tTotal Loss: 0.288670\n","Train Epoch: 1 [864/972 (89%)]\tLoss: 0.049056\tTotal Loss: 0.286240\n","Train Epoch: 1 [896/972 (92%)]\tLoss: 0.136654\tTotal Loss: 0.287470\n","Train Epoch: 1 [928/972 (95%)]\tLoss: 0.355035\tTotal Loss: 0.290650\n","Train Epoch: 1 [960/972 (99%)]\tLoss: 0.228823\tTotal Loss: 0.292500\n","Progress: 0.13 31\n","Progress: 0.26 63\n","Progress: 0.39 95\n","Progress: 0.52 127\n","Progress: 0.65 159\n","Progress: 0.79 191\n","Progress: 0.92 223\n","              precision    recall  f1-score   support\n","\n","    Negativa       0.86      0.93      0.89       476\n","      Neutra       0.48      0.38      0.42       240\n","    Positiva       0.91      0.92      0.92      1228\n","\n","    accuracy                           0.86      1944\n","   macro avg       0.75      0.74      0.74      1944\n","weighted avg       0.85      0.86      0.85      1944\n","\n","F1:  0.8503867491444849 Accuracy:  0.8569958847736625\n","Saving best model...\n","Progress: 0.13 31\n","Progress: 0.26 63\n","Progress: 0.39 95\n","Progress: 0.52 127\n","Progress: 0.65 159\n","Progress: 0.79 191\n","Progress: 0.92 223\n","              precision    recall  f1-score   support\n","\n","    Negativa       0.86      0.93      0.89       476\n","      Neutra       0.48      0.38      0.42       240\n","    Positiva       0.91      0.92      0.92      1228\n","\n","    accuracy                           0.86      1944\n","   macro avg       0.75      0.74      0.74      1944\n","weighted avg       0.85      0.86      0.85      1944\n","\n","Train Epoch: 2 [32/972 (3%)]\tLoss: 0.191462\tTotal Loss: 0.187810\n","Train Epoch: 2 [64/972 (6%)]\tLoss: 0.428084\tTotal Loss: 0.189000\n","Train Epoch: 2 [96/972 (10%)]\tLoss: 0.051636\tTotal Loss: 0.179800\n","Train Epoch: 2 [128/972 (13%)]\tLoss: 0.304681\tTotal Loss: 0.185570\n","Train Epoch: 2 [160/972 (16%)]\tLoss: 0.043653\tTotal Loss: 0.188500\n","Train Epoch: 2 [192/972 (20%)]\tLoss: 0.054658\tTotal Loss: 0.179250\n","Train Epoch: 2 [224/972 (23%)]\tLoss: 0.118430\tTotal Loss: 0.170030\n","Train Epoch: 2 [256/972 (26%)]\tLoss: 0.212764\tTotal Loss: 0.168980\n","Train Epoch: 2 [288/972 (30%)]\tLoss: 0.069824\tTotal Loss: 0.167340\n","Train Epoch: 2 [320/972 (33%)]\tLoss: 0.631517\tTotal Loss: 0.170490\n","Train Epoch: 2 [352/972 (36%)]\tLoss: 0.089288\tTotal Loss: 0.174080\n","Train Epoch: 2 [384/972 (39%)]\tLoss: 0.376172\tTotal Loss: 0.175190\n","Train Epoch: 2 [416/972 (43%)]\tLoss: 0.274404\tTotal Loss: 0.175880\n","Train Epoch: 2 [448/972 (46%)]\tLoss: 0.031139\tTotal Loss: 0.174530\n","Train Epoch: 2 [480/972 (49%)]\tLoss: 0.016076\tTotal Loss: 0.171170\n","Train Epoch: 2 [512/972 (53%)]\tLoss: 0.021821\tTotal Loss: 0.170100\n","Train Epoch: 2 [544/972 (56%)]\tLoss: 0.143572\tTotal Loss: 0.171400\n","Train Epoch: 2 [576/972 (59%)]\tLoss: 0.040819\tTotal Loss: 0.169800\n","Train Epoch: 2 [608/972 (62%)]\tLoss: 0.269729\tTotal Loss: 0.168080\n","Train Epoch: 2 [640/972 (66%)]\tLoss: 0.012943\tTotal Loss: 0.165360\n","Train Epoch: 2 [672/972 (69%)]\tLoss: 0.072941\tTotal Loss: 0.164140\n","Train Epoch: 2 [704/972 (72%)]\tLoss: 0.114084\tTotal Loss: 0.163500\n","Train Epoch: 2 [736/972 (76%)]\tLoss: 0.049586\tTotal Loss: 0.165290\n","Train Epoch: 2 [768/972 (79%)]\tLoss: 0.027842\tTotal Loss: 0.165760\n","Train Epoch: 2 [800/972 (82%)]\tLoss: 0.033398\tTotal Loss: 0.166440\n","Train Epoch: 2 [832/972 (85%)]\tLoss: 0.527255\tTotal Loss: 0.164300\n","Train Epoch: 2 [864/972 (89%)]\tLoss: 0.015094\tTotal Loss: 0.167100\n","Train Epoch: 2 [896/972 (92%)]\tLoss: 0.033373\tTotal Loss: 0.166600\n","Train Epoch: 2 [928/972 (95%)]\tLoss: 0.043552\tTotal Loss: 0.167800\n","Train Epoch: 2 [960/972 (99%)]\tLoss: 0.177888\tTotal Loss: 0.168930\n","Progress: 0.13 31\n","Progress: 0.26 63\n","Progress: 0.39 95\n","Progress: 0.52 127\n","Progress: 0.65 159\n","Progress: 0.79 191\n","Progress: 0.92 223\n","              precision    recall  f1-score   support\n","\n","    Negativa       0.89      0.88      0.88       476\n","      Neutra       0.41      0.38      0.40       240\n","    Positiva       0.90      0.92      0.91      1228\n","\n","    accuracy                           0.84      1944\n","   macro avg       0.73      0.73      0.73      1944\n","weighted avg       0.84      0.84      0.84      1944\n","\n","F1:  0.8414386832642216 Accuracy:  0.8441358024691358\n","Progress: 0.13 31\n","Progress: 0.26 63\n","Progress: 0.39 95\n","Progress: 0.52 127\n","Progress: 0.65 159\n","Progress: 0.79 191\n","Progress: 0.92 223\n","              precision    recall  f1-score   support\n","\n","    Negativa       0.89      0.88      0.88       476\n","      Neutra       0.41      0.38      0.40       240\n","    Positiva       0.90      0.92      0.91      1228\n","\n","    accuracy                           0.84      1944\n","   macro avg       0.73      0.73      0.73      1944\n","weighted avg       0.84      0.84      0.84      1944\n","\n","Train Epoch: 3 [32/972 (3%)]\tLoss: 0.424994\tTotal Loss: 0.120180\n","Train Epoch: 3 [64/972 (6%)]\tLoss: 0.069956\tTotal Loss: 0.086430\n","Train Epoch: 3 [96/972 (10%)]\tLoss: 0.069793\tTotal Loss: 0.069960\n","Train Epoch: 3 [128/972 (13%)]\tLoss: 0.010031\tTotal Loss: 0.067290\n","Train Epoch: 3 [160/972 (16%)]\tLoss: 0.056043\tTotal Loss: 0.074670\n","Train Epoch: 3 [192/972 (20%)]\tLoss: 0.714363\tTotal Loss: 0.073680\n","Train Epoch: 3 [224/972 (23%)]\tLoss: 0.422230\tTotal Loss: 0.078420\n","Train Epoch: 3 [256/972 (26%)]\tLoss: 0.179344\tTotal Loss: 0.078100\n","Train Epoch: 3 [288/972 (30%)]\tLoss: 0.015298\tTotal Loss: 0.077110\n","Train Epoch: 3 [320/972 (33%)]\tLoss: 0.029628\tTotal Loss: 0.076300\n","Train Epoch: 3 [352/972 (36%)]\tLoss: 0.006582\tTotal Loss: 0.077270\n","Train Epoch: 3 [384/972 (39%)]\tLoss: 0.051389\tTotal Loss: 0.075340\n","Train Epoch: 3 [416/972 (43%)]\tLoss: 0.007422\tTotal Loss: 0.073700\n","Train Epoch: 3 [448/972 (46%)]\tLoss: 0.009871\tTotal Loss: 0.074640\n","Train Epoch: 3 [480/972 (49%)]\tLoss: 0.021687\tTotal Loss: 0.075040\n","Train Epoch: 3 [512/972 (53%)]\tLoss: 0.013091\tTotal Loss: 0.074660\n","Train Epoch: 3 [544/972 (56%)]\tLoss: 0.033207\tTotal Loss: 0.077370\n","Train Epoch: 3 [576/972 (59%)]\tLoss: 0.064360\tTotal Loss: 0.077930\n","Train Epoch: 3 [608/972 (62%)]\tLoss: 0.024325\tTotal Loss: 0.079210\n","Train Epoch: 3 [640/972 (66%)]\tLoss: 0.011110\tTotal Loss: 0.080380\n","Train Epoch: 3 [672/972 (69%)]\tLoss: 0.018121\tTotal Loss: 0.078310\n","Train Epoch: 3 [704/972 (72%)]\tLoss: 0.005425\tTotal Loss: 0.079240\n","Train Epoch: 3 [736/972 (76%)]\tLoss: 0.009609\tTotal Loss: 0.078510\n","Train Epoch: 3 [768/972 (79%)]\tLoss: 0.008457\tTotal Loss: 0.078070\n","Train Epoch: 3 [800/972 (82%)]\tLoss: 0.007153\tTotal Loss: 0.077040\n","Train Epoch: 3 [832/972 (85%)]\tLoss: 0.342446\tTotal Loss: 0.076510\n","Train Epoch: 3 [864/972 (89%)]\tLoss: 0.101793\tTotal Loss: 0.079490\n","Train Epoch: 3 [896/972 (92%)]\tLoss: 0.127227\tTotal Loss: 0.080110\n","Train Epoch: 3 [928/972 (95%)]\tLoss: 0.004249\tTotal Loss: 0.079970\n","Train Epoch: 3 [960/972 (99%)]\tLoss: 0.011126\tTotal Loss: 0.079700\n","Progress: 0.13 31\n","Progress: 0.26 63\n","Progress: 0.39 95\n","Progress: 0.52 127\n","Progress: 0.65 159\n","Progress: 0.79 191\n","Progress: 0.92 223\n","              precision    recall  f1-score   support\n","\n","    Negativa       0.88      0.90      0.89       476\n","      Neutra       0.44      0.35      0.39       240\n","    Positiva       0.90      0.93      0.91      1228\n","\n","    accuracy                           0.85      1944\n","   macro avg       0.74      0.73      0.73      1944\n","weighted avg       0.84      0.85      0.84      1944\n","\n","F1:  0.8420356252433361 Accuracy:  0.8482510288065843\n"]}]},{"cell_type":"markdown","source":["## Predição\n","\n","Depois do treinamento do modelo, podemos usá-lo para fazer predições em novas *reviews*."],"metadata":{"id":"WC0o8orcsyjl"}},{"cell_type":"code","source":["val_X = [\"Odiei! Eu não recomendo o produto\", \"mais ou menos\", \"Adorei, vou usar muuuuuiiiiito\"]\n","val_y = []\n","\n","for x in val_X:\n","  inputs = tokenizer(x, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n","  output = model(**inputs)\n","  \n","  pred_labels = torch.argmax(output.logits, 1)\n","  \n","  val_y.append(pred_labels)\n","\n","val_y"],"metadata":{"id":"D45NnHkGs0NM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660152735565,"user_tz":180,"elapsed":16,"user":{"displayName":"Profa. Helena Caseli","userId":"17145953999266413220"}},"outputId":"c29c95f2-5025-4f40-a658-c07b3209beb8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([0], device='cuda:0'),\n"," tensor([1], device='cuda:0'),\n"," tensor([2], device='cuda:0')]"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["Testando o modelo para a sentença de exemplo do Capítulo que acompanha o minicurso."],"metadata":{"id":"xsZ5Ld2lfn0g"}},{"cell_type":"code","source":["inputs = tokenizer(\"Com estas palavras, André Coruja, além de quebrar o gelo que havia esfriado o clima, devolveu ao recinto a eloquência necessária para que a sessão continuasse.\", return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(device)\n","output = model(**inputs)\n","  \n","pred_labels = torch.argmax(output.logits, 1)\n","  \n","pred_labels\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfzBeM4he6gM","executionInfo":{"status":"ok","timestamp":1660152735565,"user_tz":180,"elapsed":4,"user":{"displayName":"Profa. Helena Caseli","userId":"17145953999266413220"}},"outputId":"3dcdfdc6-62b8-493d-a504-849e7c03e7bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2], device='cuda:0')"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["Fim deste exemplo."],"metadata":{"id":"xileRw4huaMW"}}]}